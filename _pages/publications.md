---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

**Preprint**
======
1. K. Oko, <ins>S. Akiyama</ins>, T. Murata, T. Suzuki: Versatile Single-Loop Method for Gradient Estimator: First and Second Order Optimality, and its Application to Federated Learning. [[arXiv]](https://arxiv.org/abs/2209.00361)

2. <ins>S. Akiyama</ins>, M. Obara, Y. Kawase: Optimal design of lottery with cumulative prospect theory. [[arXiv]](https://arxiv.org/abs/2209.00822)

International Conference Paper (accepted)
======
1. T. Suzuki, <ins>S. Akiyama</ins>: Benefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods. International Conference on Learning Representations 2021. (selected as splotlight). [[arXiv]](https://arxiv.org/abs/2012.03224)

2. <ins>S. Akiyama</ins>, T. Suzuki: On Learnability via Gradient Method for Two-Layer ReLU Neural Networks in Teacher-Student Setting. International Conference of Machine Learning 2021. [[arXiv]](https://arxiv.org/abs/2106.06251)

3. K. Oko, <ins>S. Akiyama</ins>, T. Murata and T. Suzuki: Reducing Communication in Nonconvex Federated Learning with a Novel Single-Loop Variance Reduction Method,  OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop), New Orleans, America, Dec. 2022.

4. <ins>S. Akiyama</ins>, T. Suzuki: Excess Risk of Two-Layer ReLU Neural Networks in Teacher-Student Settings and its Superiority to Kernel Methods. International Conference on Learning Representations 2023. [[arXiv]](https://arxiv.org/abs/2205.14818)
